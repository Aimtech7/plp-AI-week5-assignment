{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0dc185c",
   "metadata": {},
   "source": [
    "# Predicting 30-day Hospital Readmission Risk (Neural Network)\n",
    "**Dataset:** Simulated realistic hospital CSV (generated within this notebook)  \n",
    "**Model:** Multi-Layer Perceptron (MLPClassifier) with hyperparameter tuning  \n",
    "**Notebook contents:** Data generation → Full EDA → Preprocessing → Modeling → Evaluation → Saving artifacts\n",
    "\n",
    "*Author: austine makwka*  \n",
    "*Generated on: 2025-11-07*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8439e34",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "import os\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26997f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a realistic simulated hospital dataset and save as CSV\n",
    "def generate_hospital_dataset(n=5000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    patient_id = np.arange(100000, 100000 + n)\n",
    "    age = rng.randint(18, 90, size=n)\n",
    "    gender = rng.choice(['Male', 'Female'], size=n, p=[0.48, 0.52])\n",
    "    diagnoses = ['Heart Failure', 'Pneumonia', 'COPD', 'Diabetes', 'Sepsis', 'Kidney Disease', 'Stroke', 'Orthopedic']\n",
    "    diagnosis = rng.choice(diagnoses, size=n, p=[0.15,0.12,0.12,0.18,0.08,0.10,0.10,0.15])\n",
    "    num_lab_procedures = rng.poisson(10, size=n) + 1\n",
    "    num_medications = rng.poisson(5, size=n) + 1\n",
    "    time_in_hospital = np.clip(rng.poisson(4, size=n) + 1, 1, 30)\n",
    "    discharge_disposition = rng.choice(['Home', 'SNF', 'Rehab', 'Against Medical Advice', 'Expired'], size=n, p=[0.7,0.12,0.08,0.08,0.02])\n",
    "    # Simulate some comorbidity features\n",
    "    charlson_score = np.clip(rng.poisson(2, size=n) + (age/50).astype(int), 0, 15)\n",
    "    prior_admissions = rng.poisson(1.2, size=n)\n",
    "    # Social factors\n",
    "    insurance = rng.choice(['Medicare', 'Medicaid', 'Private', 'Uninsured'], size=n, p=[0.4,0.2,0.35,0.05])\n",
    "    # Base risk for readmission\n",
    "    base = 0.03 + 0.02*(prior_admissions) + 0.02*(charlson_score/10) + 0.01*(time_in_hospital/5)\n",
    "    # Diagnosis-specific risk adjustments\n",
    "    diag_risk = {'Heart Failure':0.08, 'Pneumonia':0.05, 'COPD':0.06, 'Diabetes':0.03, 'Sepsis':0.07, 'Kidney Disease':0.06, 'Stroke':0.04, 'Orthopedic':0.02}\n",
    "    diag_adj = np.array([diag_risk[d] for d in diagnosis])\n",
    "    readmit_prob = np.clip(base + diag_adj + rng.normal(0, 0.03, size=n), 0, 1)\n",
    "    readmitted = rng.binomial(1, readmit_prob)\n",
    "    df = pd.DataFrame({\n",
    "        'patient_id': patient_id,\n",
    "        'age': age,\n",
    "        'gender': gender,\n",
    "        'diagnosis': diagnosis,\n",
    "        'num_lab_procedures': num_lab_procedures,\n",
    "        'num_medications': num_medications,\n",
    "        'time_in_hospital': time_in_hospital,\n",
    "        'discharge_disposition': discharge_disposition,\n",
    "        'charlson_score': charlson_score,\n",
    "        'prior_admissions': prior_admissions,\n",
    "        'insurance': insurance,\n",
    "        'readmitted_30d': readmitted\n",
    "    })\n",
    "    # Introduce missingness\n",
    "    for col in ['charlson_score', 'insurance']:\n",
    "        idx = rng.choice(n, size=int(0.03*n), replace=False)\n",
    "        df.loc[idx, col] = np.nan\n",
    "    return df\n",
    "\n",
    "df = generate_hospital_dataset(n=4000)\n",
    "df.to_csv('hospital_data.csv', index=False)\n",
    "print('Saved hospital_data.csv with shape', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67971044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA) - Distributions and counts\n",
    "df = pd.read_csv('hospital_data.csv')\n",
    "print('Dataset shape:', df.shape)\n",
    "print('\\nReadmission rate (overall):', df['readmitted_30d'].mean())\n",
    "\n",
    "# Age distribution\n",
    "plt.hist(df['age'].dropna(), bins=20)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Time in hospital distribution\n",
    "plt.hist(df['time_in_hospital'], bins=15)\n",
    "plt.title('Length of Stay Distribution')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Number of medications distribution\n",
    "plt.hist(df['num_medications'], bins=15)\n",
    "plt.title('Number of Medications Distribution')\n",
    "plt.xlabel('Num medications')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Diagnosis frequency\n",
    "diag_counts = df['diagnosis'].value_counts()\n",
    "diag_counts.plot(kind='bar')\n",
    "plt.title('Diagnosis Frequency')\n",
    "plt.xlabel('Diagnosis')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Readmission rate by diagnosis\n",
    "readmit_by_diag = df.groupby('diagnosis')['readmitted_30d'].mean().sort_values(ascending=False)\n",
    "readmit_by_diag.plot(kind='bar')\n",
    "plt.title('Readmission Rate by Diagnosis')\n",
    "plt.ylabel('Readmission Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap (numerical features) using matplotlib only\n",
    "num_cols = ['age','num_lab_procedures','num_medications','time_in_hospital','charlson_score','prior_admissions','readmitted_30d']\n",
    "corr = df[num_cols].corr()\n",
    "print(corr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "cax = ax.matshow(corr, vmin=-1, vmax=1)\n",
    "plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha='left')\n",
    "plt.yticks(range(len(num_cols)), num_cols)\n",
    "fig.colorbar(cax)\n",
    "plt.title('Correlation Matrix (numerical features)', pad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7398b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: define pipelines for numerical and categorical features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "target = 'readmitted_30d'\n",
    "X = df.drop(columns=[target, 'patient_id'])\n",
    "y = df[target]\n",
    "\n",
    "numeric_features = ['age','num_lab_procedures','num_medications','time_in_hospital','charlson_score','prior_admissions']\n",
    "categorical_features = ['gender','diagnosis','discharge_disposition','insurance']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Run a quick transform to see feature shape\n",
    "X_prep = preprocessor.fit_transform(X)\n",
    "print('Transformed feature matrix shape:', X_prep.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f661ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test split (70/15/15 stratified)\n",
    "X_full = X\n",
    "y_full = y\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_full, y_full, test_size=0.15, stratify=y_full, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=42)\n",
    "print('Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "\n",
    "# Build pipeline with MLPClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('mlp', MLPClassifier(max_iter=300, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(32,), (64,), (64,32)],\n",
    "    'mlp__alpha': [1e-4, 1e-3],\n",
    "    'mlp__learning_rate_init': [1e-3, 5e-4]\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(pipeline, param_grid, scoring='roc_auc', cv=cv, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "print('Best params:', grid.best_params_)\n",
    "best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on validation and test sets\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "# Validation\n",
    "y_val_proba = best_model.predict_proba(X_val)[:,1]\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print('Validation AUC:', roc_auc_score(y_val, y_val_proba))\n",
    "print('Validation Precision:', precision_score(y_val, y_val_pred))\n",
    "print('Validation Recall:', recall_score(y_val, y_val_pred))\n",
    "print('Validation F1:', f1_score(y_val, y_val_pred))\n",
    "print('\\nValidation Classification Report:\\n', classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Test\n",
    "y_test_proba = best_model.predict_proba(X_test)[:,1]\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print('Test AUC:', roc_auc_score(y_test, y_test_proba))\n",
    "print('Test Precision:', precision_score(y_test, y_test_pred))\n",
    "print('Test Recall:', recall_score(y_test, y_test_pred))\n",
    "print('Test F1:', f1_score(y_test, y_test_pred))\n",
    "print('\\nTest Classification Report:\\n', classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion matrix plot\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "cax = ax.matshow(cm, cmap='Blues')\n",
    "for (i, j), z in np.ndenumerate(cm):\n",
    "    ax.text(j, i, str(z), ha='center', va='center')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Test)')\n",
    "fig.colorbar(cax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09696673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model and sample predictions; export the processed dataset subset\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "joblib.dump(best_model, 'artifacts/best_model.joblib')\n",
    "print('Saved model to artifacts/best_model.joblib')\n",
    "\n",
    "# Save a sample of processed features to CSV for inspection (transform then inverse mapping not straightforward due to one-hot)\n",
    "X_sample = X_test.copy().reset_index(drop=True)\n",
    "X_sample['pred_proba'] = y_test_proba\n",
    "X_sample['pred_label'] = y_test_pred\n",
    "X_sample.to_csv('artifacts/sample_predictions.csv', index=False)\n",
    "print('Saved sample predictions to artifacts/sample_predictions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b51f0",
   "metadata": {},
   "source": [
    "## Deployment Readiness & Reflection\n",
    "\n",
    "**Deployment plan (high level)**\n",
    "1. Wrap `artifacts/best_model.joblib` in a FastAPI service with input validation and authentication.\n",
    "2. Containerize with Docker and deploy to a secure environment (on-prem or private cloud).\n",
    "3. Integrate via FHIR/HL7 connectors or EHR middleware; show risk scores in clinician dashboard.\n",
    "4. Implement monitoring: rolling AUC, calibration, input distribution drift detection, and alerting.\n",
    "\n",
    "**Ethical considerations**\n",
    "- Ensure HIPAA compliance: encryption, access controls, BAAs.\n",
    "- Audit model fairness across demographics and perform subgroup analyses.\n",
    "- Keep clinicians in the loop; present explanations (e.g., top contributing features).\n",
    "\n",
    "**Reflection**\n",
    "- The synthetic dataset helps illustrate pipeline steps; real-world EHR data will require rigorous cleaning, de-identification, and governance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
